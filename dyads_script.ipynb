{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcd2acf-3177-4437-b7d8-6ad0448e537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import moviepy.editor as mpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b94c7512-859e-4099-b53d-0c248e64c2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(src):\n",
    "    num_labels,labels,stats,centroids = cv2.connectedComponentsWithStats(src,connectivity=4,ltype=None)\n",
    "    img = np.zeros((src.shape[0],src.shape[1]),np.uint8) # create a black background of all 0\n",
    "    for i in range(1,num_labels):\n",
    "        mask = labels == i # this step is to determine the location of the area through labels, assign labels information to the mask array, and then use the mask array as the index of img array\n",
    "        if stats[i][4] > 100: \n",
    "            img[mask] = 255 # areas larger than 100 shall be painted white, and areas smaller than 100 shall be painted black\n",
    "        else:\n",
    "            img[mask] = 0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6eddaf3-32e0-4384-a1ab-0cf5c406ccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours(frm):\n",
    "    # convert frame to grayscale\n",
    "    frm_gray = cv2.cvtColor(frm, cv2.COLOR_BGR2GRAY)\n",
    "    # remove tiny particels\n",
    "    frm_out = remove_outliers(frm_gray)\n",
    "    # convert frame to black and white\n",
    "    frm_thresh = cv2.threshold(frm_out,0,255,cv2.THRESH_BINARY)[1]\n",
    "    # extract contours\n",
    "    contours,_ = cv2.findContours(frm_thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    return contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a12d99c-4250-4d80-a2a6-aea5de235a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox_coords(clip, t=0):\n",
    "    x_bbox=y_bbox=w_bbox=h_bbox=0\n",
    "    # get the frame\n",
    "    frm = clip.get_frame(t)\n",
    "    # get the bounding box of the frame\n",
    "    contours = get_contours(frm)\n",
    "    # get the coordinates of the bounding box\n",
    "    for cnt in contours:\n",
    "        x,y,w,h = cv2.boundingRect(cnt)\n",
    "        if w > 100 and h > 100:\n",
    "            x_bbox,y_bbox,w_bbox,h_bbox=x,y,w,h\n",
    "    \n",
    "    return x_bbox,y_bbox,w_bbox,h_bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5231db6c-0e8b-4a41-a77e-61226474715c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(clip): \n",
    "    clip_x, clip_y = clip.size\n",
    "    bbox = [clip_x,0,clip_y,0]\n",
    "    average_h = 740\n",
    "    \n",
    "    # iterate through frames of the video clip\n",
    "    for frm in clip.iter_frames():\n",
    "        contours = get_contours(frm)\n",
    "        \n",
    "        # iterate over contours\n",
    "        for cnt in contours:\n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            # save rectangle that contains all boundingboxes inside it\n",
    "            if x < bbox[0]:\n",
    "                bbox[0] = x \n",
    "            if x+w > bbox[1]:\n",
    "                bbox[1] = x+w \n",
    "            if y < bbox[2]:\n",
    "                bbox[2] = y \n",
    "            if y+h > bbox[3]:\n",
    "                bbox[3] = y+h \n",
    "    \n",
    "    # crop and resize the clip\n",
    "    _,_,_,clip_h = get_bbox_coords(clip)\n",
    "    scaling_factor =  average_h/clip_h\n",
    "    \n",
    "    cropped_clip = clip.fx(mpy.vfx.crop, x1=bbox[0],x2=bbox[1],y1=bbox[2],y2=bbox[3])\n",
    "    resized_clip = cropped_clip.resize(scaling_factor)\n",
    "    \n",
    "    #calculate the center of gravity of the human body in the resized clip\n",
    "    x,y,w,h = get_bbox_coords(resized_clip)\n",
    "    center = (round((2*x+w)/2),round((2*y+h)/2))\n",
    "    \n",
    "    return resized_clip, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cecdd78e-a822-4b16-8839-a3ef3fb54715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_dur(clip, duration):\n",
    "    rate = clip.fps\n",
    "    dur = clip.duration\n",
    "    \n",
    "    desired_rate = (rate*dur)/duration\n",
    "    scaling_factor = desired_rate/rate\n",
    "\n",
    "    # Modify the FPS\n",
    "    clip = clip.set_fps(clip.fps*scaling_factor)\n",
    "    # Apply speed up\n",
    "    clip = clip.fx(mpy.vfx.speedx,scaling_factor)\n",
    "    \n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b62feb15-495b-4af9-a50a-e412de42dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_rotation(clip):\n",
    "    if clip.rotation in (90,270):\n",
    "            clip = clip.resize(clip.size[::-1])\n",
    "            clip.rotation = 0\n",
    "            \n",
    "    return clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fcc93f1-d77b-45d9-a6b6-8c8fab8a8d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dir = 'IndividualStimuli'\n",
    "files = [os.path.join(in_dir, f) for f in os.listdir(in_dir)]\n",
    "\n",
    "#out_dir = 'DyadStimuli'\n",
    "#is_existend = os.path.exists(out_dir)\n",
    "#if not is_existend:\n",
    "#    os.mkdir(out_dir)\n",
    "#    print(f'Directory \"{out_dir}\" created!')\n",
    "\n",
    "file_pairings = []\n",
    "all_pairings = []\n",
    "possible_pairings = [[('gg09', 'gg19'), ('gg01', 'gg18'), ('gg10', 'gg11'), ('gg13', 'gg17'), ('gg08', 'gg20'), ('gg04', 'gg15'), ('gg02', 'gg06'), ('gg03', 'gg07')]]\n",
    "\n",
    "#all_gestures_df = pd.read_excel('GesturesIndividualStimuli.xlsx', 'gestures_combinations')\n",
    "#relevant_gestures_df = all_gestures_df.dropna()\n",
    "#gesture_codes = relevant_gestures_df['GestureCode'].tolist()\n",
    "#gesture_combis = [(gesture_codes[i],gesture_codes[j]) for i in range(0,len(gesture_codes)) for j in range(i,len(gesture_codes)) if i != j]\n",
    "\n",
    "for gesture_combis in possible_pairings:\n",
    "    for ind,comb in enumerate(gesture_combis):\n",
    "        gestures_left = []\n",
    "        gestures_right = []\n",
    "        for f in files:\n",
    "            if re.search(rf'({comb[0]})_(f01)', f):\n",
    "                gestures_left.append(f)\n",
    "            elif re.search(rf'({comb[1]})_(f02)', f):\n",
    "                gestures_right.append(f)\n",
    "        gesture_combis[ind] = [(left,right) for right in gestures_right for left in gestures_left if left != right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4e59fec-ac0f-4577-842f-300437472e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory \"DyadPairings_1\" created!\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg09_f01_rgg19_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg09_f01_rgg19_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg09_f01_rgg19_f02.mp4\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg01_f01_rgg18_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg01_f01_rgg18_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg01_f01_rgg18_f02.mp4\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg10_f01_rgg11_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg10_f01_rgg11_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg10_f01_rgg11_f02.mp4\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg13_f01_rgg17_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg13_f01_rgg17_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg13_f01_rgg17_f02.mp4\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg08_f01_rgg20_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg08_f01_rgg20_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg08_f01_rgg20_f02.mp4\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg04_f01_rgg15_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg04_f01_rgg15_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg04_f01_rgg15_f02.mp4\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg02_f01_rgg06_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg02_f01_rgg06_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg02_f01_rgg06_f02.mp4\n",
      "Moviepy - Building video DyadPairings_1/DS_lgg03_f01_rgg07_f02.mp4.\n",
      "Moviepy - Writing video DyadPairings_1/DS_lgg03_f01_rgg07_f02.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready DyadPairings_1/DS_lgg03_f01_rgg07_f02.mp4\n"
     ]
    }
   ],
   "source": [
    "for cnt,all_pairings in enumerate(possible_pairings):\n",
    "    out_dir = f'DyadPairings_{cnt+1}'\n",
    "    is_existend = os.path.exists(out_dir)\n",
    "    if not is_existend:\n",
    "        os.mkdir(out_dir)\n",
    "        print(f'Directory \"{out_dir}\" created!')\n",
    "\n",
    "    for pairing in all_pairings:\n",
    "        for gestures in pairing:\n",
    "            # Load the video files of the individual stimuli to be combined into a dyad\n",
    "            gesture_left = mpy.VideoFileClip(gestures[0],has_mask=True)\n",
    "            gesture_right = mpy.VideoFileClip(gestures[1],has_mask=True).fx(mpy.vfx.mirror_x)\n",
    "\n",
    "            # Workaround for the moviepy bug causing videos with rotation metadata to be stretched\n",
    "            gesture_left = fix_rotation(gesture_left)\n",
    "            gesture_right = fix_rotation(gesture_right)\n",
    "\n",
    "            # Transform the individual stimuli files\n",
    "            gesture_left, center_left = crop_and_resize(gesture_left)\n",
    "            gesture_right, center_right = crop_and_resize(gesture_right)\n",
    "\n",
    "            # Speed up or slow down video to 3sec\n",
    "            duration = 3\n",
    "            gesture_right = change_dur(gesture_right,duration)\n",
    "            gesture_left = change_dur(gesture_left,duration)\n",
    "\n",
    "            # Load the background image\n",
    "            background = mpy.ImageClip('background_dyads.png',duration=duration)\n",
    "\n",
    "            # Create the video composition\n",
    "            dyad = mpy.CompositeVideoClip([background, \n",
    "                                           gesture_left.set_position((660-center_left[0],background.size[1]-gesture_left.size[1]-170)), \n",
    "                                           gesture_right.set_position((1260-center_right[0],background.size[1]-gesture_right.size[1]-170))\n",
    "                                          ])\n",
    "            # Make the video grayscale\n",
    "            dyad = dyad.fx(mpy.vfx.blackwhite)\n",
    "\n",
    "            # Write the output video file\n",
    "            out_path = os.path.join(out_dir,f'DS_l{gestures[0][21:-4]}_r{gestures[1][21:-4]}.mp4')\n",
    "            dyad.write_videofile(out_path,fps=25,audio=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dl4nlp]",
   "language": "python",
   "name": "conda-env-dl4nlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
